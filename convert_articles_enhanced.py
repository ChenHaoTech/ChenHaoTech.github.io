#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆæ–‡ç« è½¬æ¢è„šæœ¬ - åŸºäºctime/mtimeç®¡ç†Hexoæ—¥æœŸ
"""
import os
import re
import shutil
from datetime import datetime
from pathlib import Path

def parse_frontmatter_simple(content):
    """ç®€å•è§£æfrontmatterï¼Œæå–æ—¶é—´ç›¸å…³å­—æ®µ"""
    if not content.startswith('---'):
        return {}, content

    end_pos = content.find('---', 3)
    if end_pos == -1:
        return {}, content

    frontmatter_text = content[4:end_pos]
    remaining_content = content[end_pos + 3:]

    metadata = {}

    # è§£æå„ç§æ—¶é—´å­—æ®µ
    time_fields = ['ctime', 'mtime', 'created', 'updated', 'date']

    for field in time_fields:
        # åŒ¹é…æ ¼å¼: field: 2025-09-26 æˆ– field: 2025-09-26 10:06
        pattern = rf'{field}:\s*([0-9\-\s:]+)'
        match = re.search(pattern, frontmatter_text)
        if match:
            metadata[field] = match.group(1).strip()

    # è§£ææ ‡ç­¾
    tags_match = re.search(r'tags:\s*\n((?:\s+-\s+[^\n]+\n)*)', frontmatter_text)
    if tags_match:
        tags_lines = tags_match.group(1)
        tags = re.findall(r'-\s+([^\n]+)', tags_lines)
        # æ¸…ç†æ ‡ç­¾ä¸­çš„å¼•å·å’Œ#å‰ç¼€
        clean_tags = []
        for tag in tags:
            tag = tag.strip().strip('"\'').strip('#')
            if tag:
                clean_tags.append(tag)
        metadata['tags'] = clean_tags

    # è§£æå•è¡Œæ ‡ç­¾æ ¼å¼
    single_tags = re.search(r'tags:\s*\[([^\]]+)\]', frontmatter_text)
    if single_tags and 'tags' not in metadata:
        tags_str = single_tags.group(1)
        tags = [t.strip().strip('"\'').strip('#') for t in tags_str.split(',')]
        metadata['tags'] = [t for t in tags if t]

    # è§£æpublicå­—æ®µ
    public_match = re.search(r'public:\s*\n((?:\s+-\s+\w+\s*\n)*)', frontmatter_text)
    if public_match:
        public_lines = public_match.group(1)
        platforms = re.findall(r'-\s+(\w+)', public_lines)
        metadata['public'] = platforms

    return metadata, remaining_content

def parse_time_string(time_str):
    """è§£ææ—¶é—´å­—ç¬¦ä¸²ä¸ºdatetimeå¯¹è±¡"""
    if not time_str:
        return None

    # å°è¯•å„ç§æ—¶é—´æ ¼å¼
    formats = [
        '%Y-%m-%d %H:%M:%S',  # 2025-09-26 10:06:30
        '%Y-%m-%d %H:%M',     # 2025-09-26 10:06
        '%Y-%m-%d',           # 2025-09-26
        '%Y/%m/%d %H:%M:%S',  # 2025/09/26 10:06:30
        '%Y/%m/%d %H:%M',     # 2025/09/26 10:06
        '%Y/%m/%d',           # 2025/09/26
    ]

    for fmt in formats:
        try:
            return datetime.strptime(time_str.strip(), fmt)
        except ValueError:
            continue

    return None

def determine_article_date(metadata, file_path):
    """æ ¹æ®å…ƒæ•°æ®ç¡®å®šæ–‡ç« æ—¥æœŸ"""
    # ä¼˜å…ˆçº§ï¼šctime > created > mtime > updated > æ–‡ä»¶ç³»ç»Ÿæ—¶é—´

    # 1. å°è¯•ctimeï¼ˆåˆ›å»ºæ—¶é—´ï¼‰
    if 'ctime' in metadata:
        parsed_time = parse_time_string(metadata['ctime'])
        if parsed_time:
            return parsed_time, 'ctime'

    # 2. å°è¯•created
    if 'created' in metadata:
        parsed_time = parse_time_string(metadata['created'])
        if parsed_time:
            return parsed_time, 'created'

    # 3. å°è¯•mtimeï¼ˆä¿®æ”¹æ—¶é—´ï¼‰
    if 'mtime' in metadata:
        parsed_time = parse_time_string(metadata['mtime'])
        if parsed_time:
            return parsed_time, 'mtime'

    # 4. å°è¯•updated
    if 'updated' in metadata:
        parsed_time = parse_time_string(metadata['updated'])
        if parsed_time:
            return parsed_time, 'updated'

    # 5. ä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿçš„ä¿®æ”¹æ—¶é—´
    try:
        file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
        return file_mtime, 'file_mtime'
    except:
        pass

    # 6. é»˜è®¤ä½¿ç”¨å½“å‰æ—¶é—´
    return datetime.now(), 'default'

def generate_hexo_permalink(title, date):
    """ç”ŸæˆHexoé£æ ¼çš„permalink"""
    # ç”Ÿæˆå®‰å…¨çš„URL slug
    slug = re.sub(r'[^\w\u4e00-\u9fff\s-]', '', title)
    slug = re.sub(r'[-\s]+', '-', slug).strip('-').lower()

    # é™åˆ¶é•¿åº¦
    if len(slug) > 50:
        slug = slug[:50].rstrip('-')

    return f"/{date.year}/{date.month:02d}/{date.day:02d}/{slug}/"

def determine_categories(metadata, file_path):
    """æ ¹æ®æ–‡ä»¶è·¯å¾„å’Œæ ‡ç­¾ç¡®å®šåˆ†ç±»"""
    categories = []

    # åŸºäºæ–‡ä»¶è·¯å¾„ç¡®å®šä¸»åˆ†ç±»
    path_parts = Path(file_path).parts

    if 'computer' in path_parts:
        categories.append('æŠ€æœ¯æ–‡ç« ')
        # å­åˆ†ç±»
        if 'AI' in path_parts:
            categories.append('äººå·¥æ™ºèƒ½')
        elif 'ç³»ç»Ÿè®¾è®¡' in path_parts:
            categories.append('ç³»ç»Ÿæ¶æ„')
        elif 'ç®—æ³•å’Œæ•°æ®ç»“æ„' in path_parts:
            categories.append('ç®—æ³•æ•°æ®ç»“æ„')
        elif 'æ¶æ„è®¾è®¡' in path_parts:
            categories.append('æ¶æ„è®¾è®¡')
    elif 'AREA' in path_parts:
        if 'åˆ›ä¸šè®¤çŸ¥' in path_parts:
            categories.extend(['å•†ä¸šæ€è€ƒ', 'åˆ›ä¸šåˆ†æ'])
        elif 'ä¸“ä¸šåŠ›' in path_parts:
            categories.extend(['èŒä¸šå‘å±•', 'ä¸“ä¸šæˆé•¿'])
        else:
            categories.append('çŸ¥è¯†ç®¡ç†')
    elif 'Project' in path_parts:
        categories.append('é¡¹ç›®å®è·µ')
        if 'meituan' in path_parts:
            categories.append('å·¥ä½œé¡¹ç›®')
        else:
            categories.append('ä¸ªäººé¡¹ç›®')
    else:
        categories.append('å…¶ä»–')

    return categories

def convert_obsidian_to_hexo_enhanced(source_file, target_dir):
    """å¢å¼ºç‰ˆè½¬æ¢ï¼šåŸºäºæ—¶é—´å…ƒæ•°æ®ç®¡ç†æ—¥æœŸ"""

    # è¯»å–åŸæ–‡ä»¶
    with open(source_file, 'r', encoding='utf-8') as f:
        content = f.read()

    # è§£æå…ƒæ•°æ®
    metadata, main_content = parse_frontmatter_simple(content)

    # ç¡®å®šæ–‡ç« æ—¥æœŸ
    article_date, date_source = determine_article_date(metadata, source_file)

    # æå–æˆ–ç”Ÿæˆæ ‡é¢˜
    title_match = re.search(r'^# (.+)$', main_content, re.MULTILINE)
    if title_match:
        title = title_match.group(1).strip()
    else:
        title = Path(source_file).stem

    # ç¡®å®šåˆ†ç±»
    categories = determine_categories(metadata, source_file)

    # ç”Ÿæˆpermalink
    permalink = generate_hexo_permalink(title, article_date)

    # å¤„ç†æ ‡ç­¾
    tags = metadata.get('tags', ['æŠ€æœ¯åˆ†äº«'])
    if not tags:
        tags = ['æŠ€æœ¯åˆ†äº«']

    # å¤„ç†å†…å®¹ä¸­çš„é“¾æ¥
    # è½¬æ¢åŒé“¾è¯­æ³• [[æ–‡ä»¶å]] -> æ™®é€šé“¾æ¥
    main_content = re.sub(r'\[\[([^\]]+)\]\]', r'[\1](/tags/\1/)', main_content)

    # å¤„ç†å›¾ç‰‡é“¾æ¥
    main_content = re.sub(r'!\[\[([^\]]+)\]\]', r'![](/images/\1)', main_content)

    # ç”ŸæˆHexoå‰ç½®matter
    hexo_front_matter = f"""---
title: "{title}"
date: {article_date.strftime('%Y-%m-%d %H:%M:%S')}
updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
categories:"""

    # æ·»åŠ åˆ†ç±»
    for category in categories:
        hexo_front_matter += f"\n  - {category}"

    hexo_front_matter += "\ntags:"
    # æ·»åŠ æ ‡ç­¾
    for tag in tags:
        hexo_front_matter += f"\n  - {tag}"

    hexo_front_matter += f"""
permalink: {permalink}
author: é™ˆæµ©
description: "{title[:100]}..."
date_source: {date_source}
original_path: "{source_file}"
---

"""

    # æ¸…ç†å·²æœ‰çš„frontmatter
    if main_content.startswith('---'):
        end_pos = main_content.find('---', 3)
        if end_pos != -1:
            main_content = main_content[end_pos + 3:].strip()

    # ç»„åˆæœ€ç»ˆå†…å®¹
    final_content = hexo_front_matter + main_content

    # ç”Ÿæˆç›®æ ‡æ–‡ä»¶å - åŒ…å«æ—¥æœŸå‰ç¼€
    date_prefix = article_date.strftime('%Y-%m-%d')
    safe_filename = re.sub(r'[^\w\-\u4e00-\u9fff]', '-', Path(source_file).stem)
    safe_filename = re.sub(r'-+', '-', safe_filename).strip('-')
    target_filename = f"{date_prefix}-{safe_filename}.md"

    # å†™å…¥ç›®æ ‡æ–‡ä»¶
    target_path = Path(target_dir) / target_filename
    with open(target_path, 'w', encoding='utf-8') as f:
        f.write(final_content)

    return target_path, title, article_date, date_source

def generate_conversion_report(conversions):
    """ç”Ÿæˆè½¬æ¢æŠ¥å‘Š"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    # æŒ‰æ—¥æœŸæ’åº
    conversions.sort(key=lambda x: x['date'])

    # ç»Ÿè®¡ä¿¡æ¯
    total_files = len(conversions)
    date_sources = {}
    categories_count = {}

    for conv in conversions:
        source = conv['date_source']
        date_sources[source] = date_sources.get(source, 0) + 1

        for category in conv['categories']:
            categories_count[category] = categories_count.get(category, 0) + 1

    report_content = f"""# æ–‡ç« è½¬æ¢æŠ¥å‘Šï¼ˆå¢å¼ºç‰ˆï¼‰

> è½¬æ¢æ—¶é—´: {timestamp}

## ğŸ“Š è½¬æ¢ç»Ÿè®¡

- **è½¬æ¢æ–‡ä»¶**: {total_files} ç¯‡
- **æ—¶é—´è·¨åº¦**: {conversions[0]['date'].strftime('%Y-%m-%d')} è‡³ {conversions[-1]['date'].strftime('%Y-%m-%d')}

### ğŸ“… æ—¥æœŸæ¥æºåˆ†æ

"""

    for source, count in sorted(date_sources.items()):
        source_desc = {
            'ctime': 'frontmatterä¸­çš„ctimeå­—æ®µ',
            'created': 'frontmatterä¸­çš„createdå­—æ®µ',
            'mtime': 'frontmatterä¸­çš„mtimeå­—æ®µ',
            'updated': 'frontmatterä¸­çš„updatedå­—æ®µ',
            'file_mtime': 'æ–‡ä»¶ç³»ç»Ÿä¿®æ”¹æ—¶é—´',
            'default': 'å½“å‰æ—¶é—´ï¼ˆé»˜è®¤ï¼‰'
        }

        report_content += f"- **{source_desc.get(source, source)}**: {count} ç¯‡æ–‡ç« \n"

    report_content += f"""

### ğŸ·ï¸ åˆ†ç±»åˆ†å¸ƒ

"""

    for category, count in sorted(categories_count.items(), key=lambda x: x[1], reverse=True):
        report_content += f"- **{category}**: {count} ç¯‡æ–‡ç« \n"

    report_content += f"""

## ğŸ“ è¯¦ç»†è½¬æ¢è®°å½•

| åºå· | æ ‡é¢˜ | æ—¥æœŸ | æ—¥æœŸæ¥æº | åˆ†ç±» |
|------|------|------|----------|------|
"""

    for i, conv in enumerate(conversions, 1):
        categories_str = ' / '.join(conv['categories'])
        report_content += f"| {i} | {conv['title'][:30]}{'...' if len(conv['title']) > 30 else ''} | {conv['date'].strftime('%Y-%m-%d')} | {conv['date_source']} | {categories_str} |\n"

    report_content += f"""

## ğŸ”„ æ—¶é—´çº¿åˆ†æ

### æŒ‰å¹´ä»½åˆ†å¸ƒ
"""

    # æŒ‰å¹´ä»½åˆ†ç»„
    years = {}
    for conv in conversions:
        year = conv['date'].year
        years[year] = years.get(year, 0) + 1

    for year in sorted(years.keys()):
        report_content += f"- **{year}å¹´**: {years[year]} ç¯‡æ–‡ç« \n"

    report_content += f"""

### æŒ‰æœˆä»½åˆ†å¸ƒï¼ˆæœ€è¿‘ä¸€å¹´ï¼‰
"""

    # æŒ‰æœˆä»½åˆ†ç»„ï¼ˆæœ€è¿‘ä¸€å¹´ï¼‰
    recent_months = {}
    current_year = datetime.now().year

    for conv in conversions:
        if conv['date'].year == current_year:
            month_key = conv['date'].strftime('%Y-%m')
            recent_months[month_key] = recent_months.get(month_key, 0) + 1

    for month in sorted(recent_months.keys()):
        report_content += f"- **{month}**: {recent_months[month]} ç¯‡æ–‡ç« \n"

    report_content += f"""

## ğŸ’¡ å‘ç°å’Œå»ºè®®

### å†…å®¹åˆ›ä½œè¶‹åŠ¿
"""

    if years:
        max_year = max(years.keys())
        max_count = years[max_year]
        report_content += f"- æœ€æ´»è·ƒå¹´ä»½: {max_year}å¹´ ({max_count} ç¯‡æ–‡ç« )\n"

    if 'ctime' in date_sources and date_sources['ctime'] > total_files * 0.8:
        report_content += "- âœ… å¤§éƒ¨åˆ†æ–‡ç« éƒ½æœ‰å‡†ç¡®çš„åˆ›å»ºæ—¶é—´è®°å½•\n"
    elif 'file_mtime' in date_sources and date_sources['file_mtime'] > total_files * 0.5:
        report_content += "- âš ï¸  å»ºè®®ä¸ºæ–‡ç« æ·»åŠ ctimeå­—æ®µä»¥è·å¾—æ›´å‡†ç¡®çš„æ—¶é—´è®°å½•\n"

    report_content += f"""

### åˆ†ç±»å»ºè®®
"""

    if ' æŠ€æœ¯æ–‡ç« ' in categories_count and categories_count['æŠ€æœ¯æ–‡ç« '] > total_files * 0.7:
        report_content += "- ğŸ“š æŠ€æœ¯å†…å®¹ä¸°å¯Œï¼Œå»ºè®®è¿›ä¸€æ­¥ç»†åˆ†æŠ€æœ¯å­ç±»åˆ«\n"

    if len(categories_count) < 5:
        report_content += "- ğŸ”„ å†…å®¹åˆ†ç±»è¾ƒå°‘ï¼Œå»ºè®®å¢åŠ æ›´å¤šå…ƒåŒ–çš„å†…å®¹ç±»å‹\n"

    report_content += f"""

---

*æ­¤æŠ¥å‘Šç”± `enhanced-convert-articles.py` è‡ªåŠ¨ç”Ÿæˆ*
"""

    return report_content

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¢å¼ºç‰ˆæ–‡ç« è½¬æ¢è„šæœ¬ï¼ˆåŸºäºæ—¶é—´å…ƒæ•°æ®ï¼‰")
    print("="*50)

    # å®šä¹‰è·¯å¾„
    obsidian_public_dir = "../public"
    hexo_posts_dir = "source/_posts"

    # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
    Path(hexo_posts_dir).mkdir(parents=True, exist_ok=True)

    print("ğŸ“ å¼€å§‹è½¬æ¢æ–‡ç« ...")

    # éå†publicç›®å½•ä¸­çš„æ‰€æœ‰markdownæ–‡ä»¶
    public_path = Path(obsidian_public_dir)

    if not public_path.exists():
        print(f"âŒ é”™è¯¯: {obsidian_public_dir} ç›®å½•ä¸å­˜åœ¨")
        return

    conversions = []

    for md_file in public_path.glob("*.md"):
        if md_file.name.startswith('.') or md_file.name in ['README.md']:
            continue

        print(f"ğŸ“ å¤„ç†æ–‡ä»¶: {md_file.name}")

        try:
            target_path, title, article_date, date_source = convert_obsidian_to_hexo_enhanced(
                md_file, hexo_posts_dir
            )

            # ç¡®å®šåˆ†ç±»ï¼ˆé‡æ–°è°ƒç”¨ä»¥è·å–åˆ†ç±»ä¿¡æ¯ï¼‰
            if md_file.is_symlink():
                real_path = md_file.resolve()
                categories = determine_categories({}, real_path)
            else:
                categories = determine_categories({}, md_file)

            conversions.append({
                'original': md_file.name,
                'target': target_path.name,
                'title': title,
                'date': article_date,
                'date_source': date_source,
                'categories': categories
            })

            print(f"âœ… æˆåŠŸ: {title}")
            print(f"   æ—¥æœŸ: {article_date.strftime('%Y-%m-%d %H:%M')} (æ¥æº: {date_source})")
            print(f"   è¾“å‡º: {target_path.name}")

        except Exception as e:
            print(f"âŒ å¤±è´¥: {md_file.name} - {str(e)}")

    # ç”Ÿæˆè½¬æ¢æŠ¥å‘Š
    if conversions:
        report_content = generate_conversion_report(conversions)
        report_file = Path('../æ–‡ç« è½¬æ¢æŠ¥å‘Š.md')
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)

        print(f"\nğŸ“‹ è½¬æ¢æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

    print(f"\nğŸ‰ è½¬æ¢å®Œæˆ! å…±å¤„ç† {len(conversions)} ä¸ªæ–‡ä»¶")
    print(f"ğŸ“ æ–‡ç« å·²ä¿å­˜åˆ°: {hexo_posts_dir}")
    print("ğŸ’¡ ä¸‹ä¸€æ­¥: è¿è¡Œ 'npx hexo generate' ç”Ÿæˆé™æ€æ–‡ä»¶")

if __name__ == "__main__":
    main()